{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77be7dda",
   "metadata": {},
   "source": [
    "## Deploy Mixtral-8x7B model using LMI-dist of LMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1407aa-70c4-4a73-a16f-330d87bccca3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mixtral Architecture\n",
    "\n",
    "Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model, has the same architecture as Mistral 7B. The difference from Mistral is that, each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a078d",
   "metadata": {},
   "source": [
    "#### LMI-Dist \n",
    "\n",
    "The DeepSpeed container includes a library called LMI Distributed Inference Library (LMI-Dist). LMI-Dist is an inference library used to run large model inference with the best optimization used in different open-source libraries, across vLLM, Text-Generation-Inference (up to version 0.9.4), FasterTransformer, and DeepSpeed frameworks. This library incorporates open-source popular technologies like FlashAttention, PagedAttention, FusedKernel, and efficient GPU communication kernels to accelerate the model and reduce memory consumption.\n",
    "\n",
    "In this tutorial, you will use lmi-dist backend of Large Model Inference(LMI) DLC to deploy Mixtral-8x7B and run inference with it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854d769-290d-4638-a8ad-740ac1d95a37",
   "metadata": {},
   "source": [
    "#### Pre-reqs\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "* S3 bucket push access\n",
    "* SageMaker access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc171a0",
   "metadata": {},
   "source": [
    "#### Set up and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd49db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab26348",
   "metadata": {},
   "source": [
    "#### Prepare Model Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df559e",
   "metadata": {},
   "source": [
    "In LMI container, we expect some artifacts to help setting up the model\n",
    "\n",
    "* serving.properties (required): Defines the model server settings\n",
    "* model.py (optional): A python file to define the core inference logic\n",
    "* requirements.txt (optional): Any additional pip wheel need to install\n",
    "\n",
    "If you use LMI-Dist for the rolling batch option with DJL Serving, you can configure the options in the [table](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html#large-model-inference-lmi-dist) in serving.properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=MPI\n",
    "option.model_id=mistralai/Mixtral-8x7B-v0.1\n",
    "option.tensor_parallel_degree=8\n",
    "option.max_rolling_batch_size=32\n",
    "option.rolling_batch=lmi-dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf351c-92de-461d-a1f4-cc8bc2658771",
   "metadata": {},
   "source": [
    "The code and configuration you want to deploy can either be stored locally or in S3. These files will be bundled into a tar.gz file that will be uploaded to SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c21521",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a817d0e",
   "metadata": {},
   "source": [
    "#### Build SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b20125",
   "metadata": {},
   "source": [
    "#### Getting the container image URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef43453",
   "metadata": {},
   "source": [
    "Retrieve the ECR image URI for the DJL DeepSpeed accelerated large language model framework. The image URI is looked up based on the framework name, AWS region, and framework version. This allows us to dynamically select the right Docker image for our environment.\n",
    "\n",
    "Functions for generating ECR image URIs for pre-built SageMaker Docker images. See available Large Model Inference DLC's [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)\n",
    "\n",
    "\n",
    "See available Large Model Inference DLC's [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a08056",
   "metadata": {},
   "source": [
    "#### Upload artifact on S3 and create SageMaker model\n",
    "\n",
    "This code uploads a tarball containing model code to an S3 bucket under the prefix \"large-model-lmi/code\", prints the S3 path where the code was uploaded, and then creates a SageMaker model pointing to the model code that was uploaded. The model is given the S3 path to the uploaded code so that SageMaker knows where to retrieve the model code from when deploying the model. The role parameter specifies the IAM role that SageMaker can assume in order to access the uploaded model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi-dist/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c778f",
   "metadata": {},
   "source": [
    "#### Create SageMaker endpoint\n",
    "\n",
    "This cell deploys the trained model to a SageMaker endpoint for real-time inference. The instance_type defines the machine instance for the endpoint, in this case a very large. GPU instance to support fast inferences. The endpoint name is programmatically generated based on the base name. The model is deployed with a large container startup timeout specified, as the TensorRT model takes time to initialize on the GPU instance.\n",
    "\n",
    "A SageMaker Predictor is then created to call the deployed endpoint for real-time inferences. The endpoint name, sagemaker session, and JSON serializer for input/output data are specified. The predictor provides a simple interface to call the endpoint and preprocess inputs and postprocess outputs so the endpoint can be easily integrated into applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.48xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model\")\n",
    "print(f\"endpoint_name: {endpoint_name}\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             container_startup_health_check_timeout=1800\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19004557",
   "metadata": {},
   "source": [
    "#### Run inference\n",
    "\n",
    "This code snippet is making a text generation prediction using an AI model from Anthropic called Claude. The inputs key passes in the text prompt \"The future of Artificial Intelligence is\" to seed the generation. Parameters are also configured - max_new_tokens sets the maximum length to 128 tokens, while do_sample enables stochastic sampling from the model's predicted distribution during generation rather than taking the most likely token each time. The model will use the prompt and parameters to complete the sentence in a creative way, demonstrating controllable natural language generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(\n",
    "    {\"inputs\": \"The future of Artificial Intelligence is\", \"parameters\": {\"max_new_tokens\":128, \"do_sample\":True}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04ac09-8cac-4953-b339-dc8bdd6c9aba",
   "metadata": {},
   "source": [
    "#### Clean Up Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd96f5-588a-45dc-ab92-767708e0355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
